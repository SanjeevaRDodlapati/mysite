[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sanjeeva Reddy Dodlapati",
    "section": "",
    "text": "Welcome to the personal website of Sanjeeva Reddy Dodlapati. Here you can find my CV, research interests, and blog posts.\n\n\n\n\nPhD in Computer Science, Old Dominion University, Norfolk, VA, 23508\nMS Chemistry, University of New Orleans, LA\n\n\n\n\n\n\n\n\nIntern, BI, CT Jun 2018 - Aug 2018\nProject Lead, Indian Institute of Chemical Technology, Hyderabad.\n\n\n\n\n\nTeaching Assistant, Department of Computer Science, ODU, Norfolk, VA, 23508\n\n\n\n\n\n\n\nDNA methylation imputation via Transfer Learning.\nSample complexity study of DNA methylation prediction by DNA sequence-based deep learning\nDrug-Drug interaction prediction\nSynthesis of macrocyclic glycosamines by click reaction\n\n\n\n\n\n\nSanjeeva Dodlapati, ZC Jiang, J Sun, Completing Single-Cell DNA Methylome Profiles via Transfer Learning together with KL-Divergence. Frotiers in Genetics 2022\nLoss of Acta2 in cardiac fibroblasts does not prevent the myofibroblast differentiation or affect the cardiac repair after myocardial infarction. Journal of Molecular and Cellular Cardiology 171, 117-132.\nThe landscape of accessible chromatin in quiescent cardiac fibroblasts and cardiac fibroblasts activated after myocardial infarction.. Epigenetics 2021."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Sanjeeva Reddy Dodlapati",
    "section": "",
    "text": "PhD in Computer Science, Old Dominion University, Norfolk, VA, 23508\nMS Chemistry, University of New Orleans, LA"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Sanjeeva Reddy Dodlapati",
    "section": "",
    "text": "Intern, BI, CT Jun 2018 - Aug 2018\nProject Lead, Indian Institute of Chemical Technology, Hyderabad.\n\n\n\n\n\nTeaching Assistant, Department of Computer Science, ODU, Norfolk, VA, 23508"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Sanjeeva Reddy Dodlapati",
    "section": "",
    "text": "DNA methylation imputation via Transfer Learning.\nSample complexity study of DNA methylation prediction by DNA sequence-based deep learning\nDrug-Drug interaction prediction\nSynthesis of macrocyclic glycosamines by click reaction"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Sanjeeva Reddy Dodlapati",
    "section": "",
    "text": "Sanjeeva Dodlapati, ZC Jiang, J Sun, Completing Single-Cell DNA Methylome Profiles via Transfer Learning together with KL-Divergence. Frotiers in Genetics 2022\nLoss of Acta2 in cardiac fibroblasts does not prevent the myofibroblast differentiation or affect the cardiac repair after myocardial infarction. Journal of Molecular and Cellular Cardiology 171, 117-132.\nThe landscape of accessible chromatin in quiescent cardiac fibroblasts and cardiac fibroblasts activated after myocardial infarction.. Epigenetics 2021."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Research Interests",
    "section": "",
    "text": "Research Interests\nDetails about my research interests go here.\n* Deep Learning\n* Graph Learning\n* Transfer Learning\n* Multi-task Learning\n* Contrastive Learning\n* Drug Designing & Discovery\n* DNA methylation prediction"
  },
  {
    "objectID": "KGCN.html",
    "href": "KGCN.html",
    "title": "Sanjeeva Reddy Dodlapati",
    "section": "",
    "text": "Implementation of KGCN with PyTorch\n\nImport packages\n\nimport pandas as pd\nimport numpy as np\nimport argparse\nimport random\nfrom model import KGCN\nfrom data_loader import DataLoader\nimport torch\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[1], line 5\n      3 import argparse\n      4 import random\n----&gt; 5 from model import KGCN\n      6 from data_loader import DataLoader\n      7 import torch\n\nModuleNotFoundError: No module named 'model'\n\n\n\n\n\nArguments\n\n# prepare arguments (hyperparameters)\nparser = argparse.ArgumentParser()\n\nparser.add_argument('--dataset', type=str, default='music', help='which dataset to use')\nparser.add_argument('--aggregator', type=str, default='sum', help='which aggregator to use')\nparser.add_argument('--n_epochs', type=int, default=20, help='the number of epochs')\nparser.add_argument('--neighbor_sample_size', type=int, default=8, help='the number of neighbors to be sampled')\nparser.add_argument('--dim', type=int, default=16, help='dimension of user and entity embeddings')\nparser.add_argument('--n_iter', type=int, default=1, help='number of iterations when computing entity representation')\nparser.add_argument('--batch_size', type=int, default=32, help='batch size')\nparser.add_argument('--l2_weight', type=float, default=1e-4, help='weight of l2 regularization')\nparser.add_argument('--lr', type=float, default=5e-4, help='learning rate')\nparser.add_argument('--ratio', type=float, default=0.8, help='size of training dataset')\n\nargs = parser.parse_args(['--l2_weight', '1e-4'])\n\n\n\nLoad data\n\n# build dataset and knowledge graph\ndata_loader = DataLoader(args.dataset)\nkg = data_loader.load_kg()\ndf_dataset = data_loader.load_dataset()\ndf_dataset\n\n\n\nDataset Class\n\n# Dataset class\nclass KGCNDataset(torch.utils.data.Dataset):\n    def __init__(self, df):\n        self.df = df\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        user_id = np.array(self.df.iloc[idx]['userID'])\n        item_id = np.array(self.df.iloc[idx]['itemID'])\n        label = np.array(self.df.iloc[idx]['label'], dtype=np.float32)\n        return user_id, item_id, label\n\n\n\nData split\n\n# train test split\nx_train, x_test, y_train, y_test = train_test_split(df_dataset, df_dataset['label'], test_size=1 - args.ratio, shuffle=False, random_state=999)\ntrain_dataset = KGCNDataset(x_train)\ntest_dataset = KGCNDataset(x_test)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size)\n\n\n\nNetwork building\n\n# prepare network, loss function, optimizer\nnum_user, num_entity, num_relation = data_loader.get_num()\nuser_encoder, entity_encoder, relation_encoder = data_loader.get_encoders()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnet = KGCN(num_user, num_entity, num_relation, kg, args, device).to(device)\ncriterion = torch.nn.BCELoss()\noptimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2_weight)\nprint('device: ', device)\n\n\n\nTraining Network\n\n# train\nloss_list = []\ntest_loss_list = []\nauc_score_list = []\n\nfor epoch in range(args.n_epochs):\n    running_loss = 0.0\n    for i, (user_ids, item_ids, labels) in enumerate(train_loader):\n        user_ids, item_ids, labels = user_ids.to(device), item_ids.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = net(user_ids, item_ids)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        \n        optimizer.step()\n\n        running_loss += loss.item()\n    \n    # print train loss per every epoch\n    print('[Epoch {}]train_loss: '.format(epoch+1), running_loss / len(train_loader))\n    loss_list.append(running_loss / len(train_loader))\n        \n    # evaluate per every epoch\n    with torch.no_grad():\n        test_loss = 0\n        total_roc = 0\n        for user_ids, item_ids, labels in test_loader:\n            user_ids, item_ids, labels = user_ids.to(device), item_ids.to(device), labels.to(device)\n            outputs = net(user_ids, item_ids)\n            test_loss += criterion(outputs, labels).item()\n            total_roc += roc_auc_score(labels.cpu().detach().numpy(), outputs.cpu().detach().numpy())\n        print('[Epoch {}]test_loss: '.format(epoch+1), test_loss / len(test_loader))\n        test_loss_list.append(test_loss / len(test_loader))\n        auc_score_list.append(total_roc / len(test_loader))\n\n\n\nVisualization of training metrics\n\n# plot losses / scores\nfig, (ax1,ax2) = plt.subplots(1,2, figsize=(10,4))  # 1 row, 2 columns\nax1.plot(loss_list)\nax1.plot(test_loss_list)\nax2.plot(auc_score_list)\n\nplt.tight_layout()"
  },
  {
    "objectID": "hello.html",
    "href": "hello.html",
    "title": "Sanjeeva Reddy Dodlapati",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis",
    "crumbs": [
      "Home",
      "Blog Posts",
      "Demonstration of Jupyter Notebook"
    ]
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Bioinformatics",
    "section": "",
    "text": "Statistics\nWelcome to my blog. Here, I share my thoughts on various topics.\n\nT-test Applications\nTop 10 statistical concepts used in Bioiformatics"
  },
  {
    "objectID": "gve.html",
    "href": "gve.html",
    "title": "Sanjeeva Reddy Dodlapati",
    "section": "",
    "text": "import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport dash_bio\nimport scipy\nimport math\ndata = pd.read_parquet('max_gve.parquet.gzip')\ndata\n\n\n\n\n\n\n\nchrom\npos\nname\nabs_max_effect\nmax_column\nave_effect\n\n\n\n\n0\n1\n174946171\nrs147831690\n0.027101\nnrnlsc_H3K79me1\n0.000700\n\n\n1\n1\n174946180\nrs1389168246\n0.024842\nnrnlsc_H3K79me1\n0.000408\n\n\n2\n1\n174946196\nrs1374951931\n0.072019\nnrnlsc_H3K79me1\n0.001219\n\n\n3\n1\n174946202\nrs948828419\n0.033973\nnrnlsc_H3K79me1\n0.001874\n\n\n4\n1\n174946211\nrs896018724\n0.064587\nnrnlsc_H3K79me1\n0.001534\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n547681\n2\n156681080\nrs747765225\n0.009990\nSKNSH_ARNTL\n0.000462\n\n\n547682\n2\n156681082\nrs994964368\n0.009063\ncdtn_H3K4me3\n0.000822\n\n\n547683\n2\n156681084\nrs1028224262\n0.008285\nSKNSH_ARNTL\n0.000428\n\n\n547684\n2\n156681085\nrs541069773\n0.016144\nSKNSH_ARNTL\n0.000941\n\n\n547685\n2\n156681113\nrs1432010733\n0.016751\nMOC_NP_ATAC\n0.000612\n\n\n\n16476152 rows × 6 columns\ndfs = []\nfor i in range(10):\n    print(i)\n    df = data.iloc[:1000, :]\n    dfs.append(df)\nprint(df.head())\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n   chrom        pos          name  abs_max_effect       max_column  ave_effect\n0      1  174946171   rs147831690        0.027101  nrnlsc_H3K79me1    0.000700\n1      1  174946180  rs1389168246        0.024842  nrnlsc_H3K79me1    0.000408\n2      1  174946196  rs1374951931        0.072019  nrnlsc_H3K79me1    0.001219\n3      1  174946202   rs948828419        0.033973  nrnlsc_H3K79me1    0.001874\n4      1  174946211   rs896018724        0.064587  nrnlsc_H3K79me1    0.001534\ndfs = pd.concat(dfs)\ndfs\n\n\n\n\n\n\n\nchrom\npos\nname\nabs_max_effect\nmax_column\nave_effect\n\n\n\n\n0\n1\n174946171\nrs147831690\n0.027101\nnrnlsc_H3K79me1\n0.000700\n\n\n1\n1\n174946180\nrs1389168246\n0.024842\nnrnlsc_H3K79me1\n0.000408\n\n\n2\n1\n174946196\nrs1374951931\n0.072019\nnrnlsc_H3K79me1\n0.001219\n\n\n3\n1\n174946202\nrs948828419\n0.033973\nnrnlsc_H3K79me1\n0.001874\n\n\n4\n1\n174946211\nrs896018724\n0.064587\nnrnlsc_H3K79me1\n0.001534\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n1\n174949547\nrs1573971060\n0.042717\nMOC_NP_ATAC\n0.001534\n\n\n996\n1\n174949551\nrs1158195944\n0.005916\nSKNSH_H3F3A\n0.000336\n\n\n997\n1\n174949556\nrs191506112\n0.006997\nACC_NP_ATAC\n0.000302\n\n\n998\n1\n174949557\nrs557939733\n0.005723\nACC_NP_ATAC\n0.000226\n\n\n999\n1\n174949559\nrs551299464\n0.007402\nMiddleFrGy_DNase\n0.000275\n\n\n\n10000 rows × 6 columns\ndata = pd.read_csv('max_gve.csv')\ndata['chrom'][data.chrom=='X'] = 23\ndata['chrom'][data.chrom=='Y'] = 24\ndata['chrom'][data.chrom=='16.0'] = 16\ndata['chrom'] = data.chrom.astype(int)\ndata\n\n\n\n\n\n\n\nchrom\npos\nname\nmax_effect\nmax_column\nave_effect\n\n\n\n\n0\n1\n238807165\nrs1207962803\n0.015620\nbrnmec_DNase\n0.000920\n\n\n1\n1\n238807165\nrs1242649293\n0.081236\nnrnlsc_H3K9ac\n-0.002660\n\n\n2\n1\n238807167\nrs997302978\n0.002643\nACC_NP_ATAC\n-0.001335\n\n\n3\n1\n238807168\nrs1270214146\n0.045428\nnrnlsc_H3K79me1\n-0.000773\n\n\n4\n1\n238807168\nrs1270214146\n0.083614\nnrnlsc_H3K79me1\n0.001402\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n16652518\n2\n156681080\nrs747765225\n0.006044\nPMC_NN_ATAC\n-0.000144\n\n\n16652519\n2\n156681082\nrs994964368\n0.000293\nMeT_NN_ATAC\n-0.000791\n\n\n16652520\n2\n156681084\nrs1028224262\n0.004548\nVPC_NN_ATAC\n-0.000109\n\n\n16652521\n2\n156681085\nrs541069773\n0.011259\nVPC_NN_ATAC\n-0.000122\n\n\n16652522\n2\n156681113\nrs1432010733\n0.010121\nnrnlsc_H3K4me1\n0.000242\n\n\n\n16652523 rows × 6 columns\ndata.chrom.unique()\n\narray([ 1, 22,  3, 12, 16, 17, 13, 10, 11,  2, 19, 14, 15, 21])\nneg = -(data.max_effect.values)\nexp = np.exp(neg)\ndata['exp'] = exp\n\nzscores = scipy.stats.zscore(data.ave_effect.values)\ndata['zscore'] = zscores\npvalues = scipy.stats.norm.sf(zscores)\ndata['p'] = pvalues\n\ndata\n\n\n\n\n\n\n\nchrom\npos\nname\nmax_effect\nmax_column\nave_effect\nexp\nzscore\np\n\n\n\n\n0\n1\n238807165\nrs1207962803\n0.015620\nbrnmec_DNase\n0.000920\n0.984502\n0.310131\n0.378231\n\n\n1\n1\n238807165\nrs1242649293\n0.081236\nnrnlsc_H3K9ac\n-0.002660\n0.921976\n-0.931275\n0.824144\n\n\n2\n1\n238807167\nrs997302978\n0.002643\nACC_NP_ATAC\n-0.001335\n0.997360\n-0.471795\n0.681464\n\n\n3\n1\n238807168\nrs1270214146\n0.045428\nnrnlsc_H3K79me1\n-0.000773\n0.955588\n-0.276915\n0.609077\n\n\n4\n1\n238807168\nrs1270214146\n0.083614\nnrnlsc_H3K79me1\n0.001402\n0.919786\n0.477353\n0.316555\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16652518\n2\n156681080\nrs747765225\n0.006044\nPMC_NN_ATAC\n-0.000144\n0.993974\n-0.058674\n0.523394\n\n\n16652519\n2\n156681082\nrs994964368\n0.000293\nMeT_NN_ATAC\n-0.000791\n0.999707\n-0.283229\n0.611499\n\n\n16652520\n2\n156681084\nrs1028224262\n0.004548\nVPC_NN_ATAC\n-0.000109\n0.995462\n-0.046438\n0.518520\n\n\n16652521\n2\n156681085\nrs541069773\n0.011259\nVPC_NN_ATAC\n-0.000122\n0.988804\n-0.050923\n0.520306\n\n\n16652522\n2\n156681113\nrs1432010733\n0.010121\nnrnlsc_H3K4me1\n0.000242\n0.989930\n0.075191\n0.470031\n\n\n\n16652523 rows × 9 columns\ndata.to_csv('abs_max_ave_effect_pvalue.csv', index=False)\ndata = pd.read_csv('tbln_H3K27ac_kmeans0.csv')\ndata['chrom'][data.chrom=='X'] = 23\ndata['chrom'][data.chrom=='Y'] = 24\ndata['chrom'][data.chrom=='16.0'] = 16\ndata['chrom'] = data.chrom.astype(int)\ndata\n\n\n\n\n\n\n\nchrom\npos\nname\ntbln_H3K27ac\ndiff\nabs_diff\n\n\n\n\n0\n10\n84139514.0\nrs932821133\n0.105500\n0.105500\n0.105500\n\n\n1\n10\n84139543.0\nrs1229596357\n-0.183456\n-0.183456\n0.183456\n\n\n2\n10\n84139573.0\nrs1481839079\n0.143182\n0.143182\n0.143182\n\n\n3\n10\n84140064.0\nrs1452183496\n0.136644\n0.136644\n0.136644\n\n\n4\n10\n84140294.0\nrs1429667538\n0.100358\n0.100358\n0.100358\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n726226\n3\n194089744.0\nrs1351149363\n-0.218239\n-0.218239\n0.218239\n\n\n726227\n3\n194089846.0\nrs890061713\n-0.116548\n-0.116548\n0.116548\n\n\n726228\n3\n194089911.0\nrs1246306491\n-0.164336\n-0.164336\n0.164336\n\n\n726229\n3\n194090129.0\nrs1181593567\n-0.111510\n-0.111510\n0.111510\n\n\n726230\n3\n194095916.0\nrs1009672135\n-0.116104\n-0.116104\n0.116104\n\n\n\n726231 rows × 6 columns\nneg = -(data.tbln_H3K27ac.values)\nexp = np.exp(neg)\ndata['exp'] = exp\n\nzscores = scipy.stats.zscore(data.tbln_H3K27ac.values)\ndata['zscore'] = zscores\npvalues = scipy.stats.norm.sf(zscores)\ndata['p'] = pvalues\n\ndata\n\n\n\n\n\n\n\nchrom\npos\nname\ntbln_H3K27ac\ndiff\nabs_diff\nexp\n\n\n\n\n0\n10\n84139514.0\nrs932821133\n0.105500\n0.105500\n0.105500\n0.899874\n\n\n1\n10\n84139543.0\nrs1229596357\n-0.183456\n-0.183456\n0.183456\n1.201363\n\n\n2\n10\n84139573.0\nrs1481839079\n0.143182\n0.143182\n0.143182\n0.866597\n\n\n3\n10\n84140064.0\nrs1452183496\n0.136644\n0.136644\n0.136644\n0.872280\n\n\n4\n10\n84140294.0\nrs1429667538\n0.100358\n0.100358\n0.100358\n0.904514\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n726226\n3\n194089744.0\nrs1351149363\n-0.218239\n-0.218239\n0.218239\n1.243884\n\n\n726227\n3\n194089846.0\nrs890061713\n-0.116548\n-0.116548\n0.116548\n1.123611\n\n\n726228\n3\n194089911.0\nrs1246306491\n-0.164336\n-0.164336\n0.164336\n1.178611\n\n\n726229\n3\n194090129.0\nrs1181593567\n-0.111510\n-0.111510\n0.111510\n1.117965\n\n\n726230\n3\n194095916.0\nrs1009672135\n-0.116104\n-0.116104\n0.116104\n1.123113\n\n\n\n726231 rows × 7 columns\nplt.hist(data.tbln_H3K27ac)\n\n(array([2.40000e+01, 5.00000e+02, 1.43580e+04, 3.43638e+05, 0.00000e+00,\n        3.38365e+05, 2.81640e+04, 1.12700e+03, 4.00000e+01, 1.50000e+01]),\n array([-0.74844134, -0.58586169, -0.42328205, -0.2607024 , -0.09812275,\n         0.0644569 ,  0.22703654,  0.38961619,  0.55219584,  0.71477548,\n         0.87735513]),\n &lt;BarContainer object of 10 artists&gt;)\n# scipy.stats.t.sf(data.ITC_NN_ATAC.values, df=2)\n# scipy.stats.t.sf(data.ITC_NN_ATAC.values, df=3)\nchrom\npos\nname\ntbln_H3K27ac\ndiff\nabs_diff\nexp\nzscore\np\n\n\n\n\n0\n10\n84139514.0\nrs932821133\n0.105500\n0.105500\n0.105500\n0.899874\n0.658540\n0.255096\n\n\n1\n10\n84139543.0\nrs1229596357\n-0.183456\n-0.183456\n0.183456\n1.201363\n-1.181742\n0.881346\n\n\n2\n10\n84139573.0\nrs1481839079\n0.143182\n0.143182\n0.143182\n0.866597\n0.898519\n0.184454\n\n\n3\n10\n84140064.0\nrs1452183496\n0.136644\n0.136644\n0.136644\n0.872280\n0.856886\n0.195754\n\n\n4\n10\n84140294.0\nrs1429667538\n0.100358\n0.100358\n0.100358\n0.904514\n0.625787\n0.265727\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n726226\n3\n194089744.0\nrs1351149363\n-0.218239\n-0.218239\n0.218239\n1.243884\n-1.403260\n0.919730\n\n\n726227\n3\n194089846.0\nrs890061713\n-0.116548\n-0.116548\n0.116548\n1.123611\n-0.755621\n0.775062\n\n\n726228\n3\n194089911.0\nrs1246306491\n-0.164336\n-0.164336\n0.164336\n1.178611\n-1.059971\n0.855421\n\n\n726229\n3\n194090129.0\nrs1181593567\n-0.111510\n-0.111510\n0.111510\n1.117965\n-0.723538\n0.765325\n\n\n726230\n3\n194095916.0\nrs1009672135\n-0.116104\n-0.116104\n0.116104\n1.123113\n-0.752797\n0.774214\n\n\n\n726231 rows × 9 columns\ndata.chrom.unique()\n\narray([10,  1, 16, 23, 15, 21,  2,  6,  7,  5, 12, 19,  8,  3,  9,  4, 11,\n       24, 13, 14, 17, 22, 20, 18])\ndata.to_csv('data_with_pvalue.csv', index=False)\ndash_bio.ManhattanPlot(dataframe=df, chrm='CHR', bp='BP', snp='SNP', p='P')\nfile_lists = [files[:500], files[500:1000], files[1000:1500], files[1500:2000], files[2000:2500], files[2500:3000],\n              files[3000:3500], files[3500:4000], files[4000:4500], files[4500:5000], files[5000:]]\ndf = pd.read_csv('https://raw.githubusercontent.com/plotly/dash-bio-docs-files/master/manhattan_data.csv')\ndf\n\n\n\n\n\n\n\nCHR\nBP\nP\nSNP\nZSCORE\nEFFECTSIZE\nGENE\nDISTANCE\n\n\n\n\n0\n1\n937641\n0.335344\nrs9697358\n0.9634\n-0.0946\nISG15\n1068\n\n\n1\n1\n1136887\n0.245857\nrs34945898\n1.1605\n-0.0947\nTNFRSF4\n0\n\n\n2\n1\n2116240\n0.823286\nrs12034613\n0.2233\n-0.0741\nFP7162\n0\n\n\n3\n1\n2310562\n0.493204\nrs4648633\n0.6852\n0.0146\nMORN1\n0\n\n\n4\n1\n2681715\n0.605392\nrs4430271\n0.5167\n0.1234\nMMEL1\n127427\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n14407\n23\n153207974\n0.574920\nrs766418\n0.5608\n-0.0190\nNaN\n-1\n\n\n14408\n23\n153280339\n0.978400\nrs11593\n0.0271\n-0.1355\nNaN\n-1\n\n\n14409\n23\n153546061\n0.056012\nrs5987005\n1.9109\n0.0026\nNaN\n-1\n\n\n14410\n23\n153903746\n0.132189\nrs28370194\n1.5055\n-0.1425\nNaN\n-1\n\n\n14411\n23\n154208789\n0.497909\nrs524609\n0.6778\n0.2542\nNaN\n-1\n\n\n\n14412 rows × 8 columns\n# dash_bio.ManhattanPlot(dataframe=data, chrm='CHR', bp='pos')\nfiles = [i for i in range(5561)]\nlen(files)\n\n5561\ni = 0\nstep = 500\nfile_lists = []\nwhile i+step &lt;= len(files):\n    print(i)\n    flist = files[i:i+step]\n    i+=step\n    file_lists.append(flist)\n    \nflist = files[i:]\nfile_lists.append(flist)\nprint(len(file_lists))\n\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\n5000\n12\nlen(file_lists[-1])\n\n61"
  },
  {
    "objectID": "hello.html#meet-quarto",
    "href": "hello.html#meet-quarto",
    "title": "Hello, Quarto",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "hello.html#meet-the-penguins",
    "href": "hello.html#meet-the-penguins",
    "title": "Hello, Quarto",
    "section": "Meet the penguins",
    "text": "Meet the penguins\n\nThe penguins data from the palmerpenguins package contains size measurements for 344 penguins from three species observed on three islands in the Palmer Archipelago, Antarctica.\nThe plot below shows the relationship between flipper and bill lengths of these penguins."
  },
  {
    "objectID": "bioNumpy.html",
    "href": "bioNumpy.html",
    "title": "BioNumpy",
    "section": "",
    "text": "Here’s the content formatted to be directly copied and pasted into a .qmd file:\nApplication: Efficiently handle DNA sequencing data from FASTA files for genomic analyses, such as finding reverse complements in large genomic datasets."
  },
  {
    "objectID": "bioNumpy.html#one-hot-encoding",
    "href": "bioNumpy.html#one-hot-encoding",
    "title": "BioNumpy",
    "section": "2. One-Hot Encoding",
    "text": "2. One-Hot Encoding\n# Convert a DNA sequence to one-hot encoding\none_hot_seq = bnp.one_hot_encode(\"ACGTGCA\")\n\n# Print encoded array\nprint(one_hot_seq)\nApplication: One-hot encoding is commonly used as input for machine learning models that predict gene functions, allowing the model to interpret sequence data numerically."
  },
  {
    "objectID": "bioNumpy.html#vectorized-operations",
    "href": "bioNumpy.html#vectorized-operations",
    "title": "BioNumpy",
    "section": "3. Vectorized Operations",
    "text": "3. Vectorized Operations\n# Vectorized slicing to get subsequences\nsubsequences = sequences[:, :10]  # Get the first 10 nucleotides from each sequence\n\n# Filtering sequences with a specific nucleotide count\nhigh_gc_sequences = sequences[bnp.gc_content(sequences) &gt; 0.5]\nApplication: Quickly extract or filter specific segments of sequences for analysis, such as selecting sequences with high GC content, which might indicate certain genomic regions."
  },
  {
    "objectID": "bioNumpy.html#handling-of-biological-data-formats",
    "href": "bioNumpy.html#handling-of-biological-data-formats",
    "title": "BioNumpy",
    "section": "4. Handling of Biological Data Formats",
    "text": "4. Handling of Biological Data Formats\n# Read sequences from FASTQ and write to FASTA\nfastq_sequences = bnp.read_fastq(\"example.fastq\")\nbnp.write_fasta(\"converted.fasta\", fastq_sequences)\nApplication: Convert sequencing reads from FASTQ to FASTA format for downstream analysis, like quality control, mapping, or assembly workflows."
  },
  {
    "objectID": "bioNumpy.html#alphabet-encoding-and-mapping",
    "href": "bioNumpy.html#alphabet-encoding-and-mapping",
    "title": "BioNumpy",
    "section": "5. Alphabet Encoding and Mapping",
    "text": "5. Alphabet Encoding and Mapping\n# Map nucleotide sequences to integer arrays\nencoded_seq = bnp.map_to_alphabet(\"ACGTGCA\", alphabet=bnp.dna_alphabet)\n\n# Print encoded sequence\nprint(encoded_seq)\nApplication: Quickly convert sequences to integer-encoded arrays for faster comparisons or to use as input to statistical algorithms or machine learning models."
  },
  {
    "objectID": "bioNumpy.html#gc-content-and-sequence-statistics",
    "href": "bioNumpy.html#gc-content-and-sequence-statistics",
    "title": "BioNumpy",
    "section": "6. GC Content and Sequence Statistics",
    "text": "6. GC Content and Sequence Statistics\n# Calculate GC content of sequences\ngc_content = bnp.gc_content(sequences)\nprint(\"GC Content:\", gc_content)\nApplication: Use GC content to identify GC-rich or GC-poor regions, which can indicate functional genomic elements like promoters, exons, or repetitive regions."
  },
  {
    "objectID": "bioNumpy.html#parallel-processing-support",
    "href": "bioNumpy.html#parallel-processing-support",
    "title": "BioNumpy",
    "section": "7. Parallel Processing Support",
    "text": "7. Parallel Processing Support\nfrom concurrent.futures import ProcessPoolExecutor\n\n# Process sequences in parallel\nwith ProcessPoolExecutor() as executor:\n    results = list(executor.map(bnp.gc_content, sequences))\nApplication: Efficiently calculate metrics like GC content on large datasets by utilizing multiple CPU cores, reducing processing time."
  },
  {
    "objectID": "bioNumpy.html#integration-with-other-python-packages",
    "href": "bioNumpy.html#integration-with-other-python-packages",
    "title": "BioNumpy",
    "section": "8. Integration with Other Python Packages",
    "text": "8. Integration with Other Python Packages\nimport pandas as pd\n\n# Convert sequences and GC content to DataFrame\ngc_df = pd.DataFrame({\n    'Sequence': sequences,\n    'GC_Content': bnp.gc_content(sequences)\n})\n\n# Analyze using Pandas functions\nhigh_gc_df = gc_df[gc_df['GC_Content'] &gt; 0.5]\nprint(high_gc_df)\nApplication: Use BioNumPy with Pandas to perform advanced filtering, group-by operations, and aggregations for in-depth genomic analysis and visualization."
  },
  {
    "objectID": "bioNumpy.html#support-for-sequence-alignment-and-similarity-measures",
    "href": "bioNumpy.html#support-for-sequence-alignment-and-similarity-measures",
    "title": "BioNumpy",
    "section": "9. Support for Sequence Alignment and Similarity Measures",
    "text": "9. Support for Sequence Alignment and Similarity Measures\n# Calculate sequence similarity\nsimilarity_score = bnp.sequence_similarity(\"ACGT\", \"AGGT\")\nprint(\"Similarity Score:\", similarity_score)\nApplication: Sequence similarity scores are essential in tasks like identifying homologous sequences in different species, detecting conserved motifs, or clustering similar sequences."
  },
  {
    "objectID": "bioNumpy.html#sequence-motif-search",
    "href": "bioNumpy.html#sequence-motif-search",
    "title": "BioNumpy",
    "section": "10. Sequence Motif Search",
    "text": "10. Sequence Motif Search\n# Search for a motif in a DNA sequence\nmotif = \"GATA\"\nmatches = bnp.find_motif(sequences, motif)\nprint(\"Motif Matches:\", matches)\nApplication: Motif search is essential for tasks like identifying transcription factor binding sites, RNA binding motifs, or repeat elements within genomic sequences, often used in regulatory genomics. ```"
  },
  {
    "objectID": "bioNumpy.html#each-section-includes-a-code-block-an-explanation-and-a-practical-application-making-it-ready-for-documentation-or-tutorials.",
    "href": "bioNumpy.html#each-section-includes-a-code-block-an-explanation-and-a-practical-application-making-it-ready-for-documentation-or-tutorials.",
    "title": "BioNumpy",
    "section": "Each section includes a code block, an explanation, and a practical application, making it ready for documentation or tutorials.",
    "text": "Each section includes a code block, an explanation, and a practical application, making it ready for documentation or tutorials."
  },
  {
    "objectID": "Untitled1.html",
    "href": "Untitled1.html",
    "title": "Sanjeeva Reddy Dodlapati",
    "section": "",
    "text": "import numpy as np\nimport bionumpy as bnp\n\n# Create an array of nucleotide sequences as strings for testing\n# Each string in the array represents a simple DNA sequence\nsequences = np.array([\"ACGTGCA\", \"GATTACA\", \"CCCGGGTTT\"])\n\n# Example: reverse complement each sequence\nreverse_complements = [bnp.reverse_complement(seq) for seq in sequences]\nprint(\"Reverse Complements:\", reverse_complements)\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[4], line 9\n      6 sequences = np.array([\"ACGTGCA\", \"GATTACA\", \"CCCGGGTTT\"])\n      8 # Example: reverse complement each sequence\n----&gt; 9 reverse_complements = [bnp.reverse_complement(seq) for seq in sequences]\n     10 print(\"Reverse Complements:\", reverse_complements)\nAttributeError: module 'bionumpy' has no attribute 'reverse_complement'\n\n\n\n\nimport numpy as np\nimport bionumpy as bnp\n\n# Define possible nucleotides\nnucleotides = np.array([\"A\", \"C\", \"G\", \"T\"])\n\n# Generate an array of random nucleotide sequences\nsequence_length = 10\nnum_sequences = 5\nrandom_sequences = [\"\".join(np.random.choice(nucleotides, sequence_length)) for _ in range(num_sequences)]\nprint(\"Random Sequences:\", random_sequences)\n\n# Use BioNumPy to calculate GC content for each random sequence\ngc_contents = [bnp.gc_content(seq) for seq in random_sequences]\nprint(\"GC Contents:\", gc_contents)\n\nRandom Sequences: ['GGATGGAACT', 'GATTGTTTCC', 'ACCACAATTT', 'GATACCTAAG', 'GACCGTCAGC']\n\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[5], line 14\n     11 print(\"Random Sequences:\", random_sequences)\n     13 # Use BioNumPy to calculate GC content for each random sequence\n---&gt; 14 gc_contents = [bnp.gc_content(seq) for seq in random_sequences]\n     15 print(\"GC Contents:\", gc_contents)\nAttributeError: module 'bionumpy' has no attribute 'gc_content'"
  },
  {
    "objectID": "T-test.html",
    "href": "T-test.html",
    "title": "Understanding T-Tests: A Comprehensive Guide to Comparing Group Means",
    "section": "",
    "text": "A t-test is a statistical method used to determine whether there is a significant difference between the means of two groups. It’s commonly applied in various fields to compare group averages and assess the impact of interventions or treatments.\nPurpose: The primary goal of a t-test is to evaluate whether the observed differences between group means are statistically significant or if they could have occurred by chance. For example, it can help determine if a new teaching method leads to higher test scores compared to a traditional approach.\nApplications: - Education: Comparing average test scores between two classes to assess different teaching methods. - Medicine: Evaluating the effectiveness of a new drug by comparing patient outcomes between a treatment group and a control group. - Business: Assessing whether a new marketing strategy leads to higher sales compared to the previous strategy.\nTypes of T-Tests: 1. Independent Samples T-Test: This test compares the means of two separate groups to see if they differ significantly. For instance, comparing the average heights of men and women. 2. Paired Samples T-Test: This test compares the means from the same group at two different times or under two different conditions. An example would be measuring the weight of individuals before and after a diet program.\nAssumptions: For the results of a t-test to be valid, certain assumptions should be met: - Normality: The data in each group should be approximately normally distributed. This means that when plotted, the data should form a bell-shaped curve. - Equal Variances: The variability (spread) of scores in the two groups should be similar. This assumption is known as homogeneity of variances.\nIf these assumptions are violated, the results of the t-test may not be reliable. In such cases, alternative statistical methods or data transformations might be necessary.\nUnderstanding and correctly applying t-tests enable researchers and analysts to make informed decisions based on data, ensuring that observed differences between groups are meaningful and not due to random chance.\nTo demonstrate the applications of t-tests in Python, we’ll use the scipy.stats module, which provides functions for performing various statistical tests. Below are examples for both Independent Samples T-Test and Paired Samples T-Test, along with sample data.\n\nIndependent Samples T-Test\n\nThis test compares the means of two independent groups to determine if they are significantly different. For instance, comparing the average test scores of two different classes.\nExample:\nSuppose we have test scores from two classes, and we want to determine if there’s a significant difference between their average scores.\n\nimport numpy as np\nfrom scipy import stats\n\n# Sample data: test scores of two classes\nclass_A_scores = [85, 88, 90, 92, 86, 87, 91, 89, 84, 90]\nclass_B_scores = [78, 82, 80, 79, 81, 77, 83, 80, 78, 82]\n\n# Perform Independent Samples T-Test\nt_stat, p_value = stats.ttest_ind(class_A_scores, class_B_scores)\n\nprint(f\"T-Statistic: {t_stat:.2f}\")\nprint(f\"P-Value: {p_value:.4f}\")\n\n# Interpret the result\nalpha = 0.05\nif p_value &lt; alpha:\n    print(\"Reject the null hypothesis: There is a significant difference between the two classes.\")\nelse:\n    print(\"Fail to reject the null hypothesis: No significant difference between the two classes.\")\n\nT-Statistic: 7.79\nP-Value: 0.0000\nReject the null hypothesis: There is a significant difference between the two classes.\n\n\nIn this example, the p-value is less than the significance level (alpha = 0.05), indicating a significant difference between the average scores of the two classes.\n\nPaired Samples T-Test\n\nThis test compares the means from the same group at different times or under different conditions. For example, measuring the weights of individuals before and after a diet program.\nExample:\nAssume we have the weights of individuals before and after a diet program, and we want to determine if the program had a significant effect.\n\nimport numpy as np\nfrom scipy import stats\n\n# Sample data: weights before and after a diet program\nweights_before = [200, 195, 180, 210, 190, 205, 185, 200, 195, 210]\nweights_after = [190, 188, 175, 200, 185, 198, 180, 195, 190, 205]\n\n# Perform Paired Samples T-Test\nt_stat, p_value = stats.ttest_rel(weights_before, weights_after)\n\nprint(f\"T-Statistic: {t_stat:.2f}\")\nprint(f\"P-Value: {p_value:.4f}\")\n\n# Interpret the result\nalpha = 0.05\nif p_value &lt; alpha:\n    print(\"Reject the null hypothesis: The diet program had a significant effect.\")\nelse:\n    print(\"Fail to reject the null hypothesis: No significant effect of the diet program.\")\n\nT-Statistic: 9.80\nP-Value: 0.0000\nReject the null hypothesis: The diet program had a significant effect.\n\n\nHere, the p-value is less than 0.05, suggesting that the diet program led to a significant reduction in weight.\nNote: Before performing t-tests, it’s essential to check the assumptions of normality and equal variances. If these assumptions are violated, consider using non-parametric tests or applying data transformations.\nThese examples illustrate how to perform t-tests in Python using sample data, helping to determine whether observed differences between groups are statistically significant."
  },
  {
    "objectID": "Top10_bioinfo_stats.html",
    "href": "Top10_bioinfo_stats.html",
    "title": "Frequently used statistical concepts in Bioinformatics",
    "section": "",
    "text": "import os\nimport pandas as pd\nimport numpy as np\nimport random\nimport scipy as scp\nimport warnings\nwarnings.filterwarnings('ignore')\n# # Install packages. Uncomment to install and comment back after installing\n# %pip install hmmlearn\n# %pip install lifelines\nIn bioinformatics, statistical concepts are pivotal for analyzing and interpreting complex biological data. Below are ten fundamental statistical concepts, each defined with explanations, mathematical formulations, and Python code examples to illustrate their applications."
  },
  {
    "objectID": "hello.html#polar-axis",
    "href": "hello.html#polar-axis",
    "title": "Sanjeeva Reddy Dodlapati",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis",
    "crumbs": [
      "Home",
      "Blog Posts",
      "Demonstration of Jupyter Notebook"
    ]
  },
  {
    "objectID": "T-test-alt.html",
    "href": "T-test-alt.html",
    "title": "Student t-test: Applications",
    "section": "",
    "text": "# Define the URL as a Python variable\nurl = \"https://www.reddydodlapati.com/top10_bioinfo_stats\"\n\n# Display the HTML using Python's f-string\nfrom IPython.display import display, HTML\n\nhtml_code = f\"\"\"\n&lt;div class=\"social-share\"&gt;\n  &lt;a href=\"https://twitter.com/intent/tweet?text=Check out this blog post&url={url}\" target=\"_blank\"&gt;\n    &lt;img src=\"https://cdn-icons-png.flaticon.com/512/733/733579.png\" alt=\"Share on Twitter\" width=\"24px\"&gt;\n  &lt;/a&gt;\n  &lt;a href=\"https://www.linkedin.com/sharing/share-offsite/?url={url}\" target=\"_blank\"&gt;\n    &lt;img src=\"https://cdn-icons-png.flaticon.com/512/733/733561.png\" alt=\"Share on LinkedIn\" width=\"24px\"&gt;\n  &lt;/a&gt;\n  &lt;a href=\"https://www.facebook.com/sharer/sharer.php?u={url}\" target=\"_blank\"&gt;\n    &lt;img src=\"https://cdn-icons-png.flaticon.com/512/733/733547.png\" alt=\"Share on Facebook\" width=\"24px\"&gt;\n  &lt;/a&gt;\n&lt;/div&gt;\n\"\"\"\n\ndisplay(HTML(html_code))\n\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n\n\n\n\nUnderstanding T-Tests: A Comprehensive Guide to Comparing Group Means\nA t-test is a statistical method used to determine whether there is a significant difference between the means of two groups. It’s commonly applied in various fields to compare group averages and assess the impact of interventions or treatments.\nPurpose: The primary goal of a t-test is to evaluate whether the observed differences between group means are statistically significant or if they could have occurred by chance. For example, it can help determine if a new teaching method leads to higher test scores compared to a traditional approach.\nApplications: - Education: Comparing average test scores between two classes to assess different teaching methods.\n\nMedicine: Evaluating the effectiveness of a new drug by comparing patient outcomes between a treatment group and a control group.\nBusiness: Assessing whether a new marketing strategy leads to higher sales compared to the previous strategy.\n\nTypes of T-Tests:\n\nIndependent Samples T-Test: This test compares the means of two separate groups to see if they differ significantly. For instance, comparing the average heights of men and women.\nPaired Samples T-Test: This test compares the means from the same group at two different times or under two different conditions. An example would be measuring the weight of individuals before and after a diet program.\n\nAssumptions: For the results of a t-test to be valid, certain assumptions should be met:\n\nNormality: The data in each group should be approximately normally distributed. This means that when plotted, the data should form a bell-shaped curve.\nEqual Variances: The variability (spread) of scores in the two groups should be similar. This assumption is known as homogeneity of variances.\n\nIf these assumptions are violated, the results of the t-test may not be reliable. In such cases, alternative statistical methods or data transformations might be necessary.\nUnderstanding and correctly applying t-tests enable researchers and analysts to make informed decisions based on data, ensuring that observed differences between groups are meaningful and not due to random chance.\nTo demonstrate the applications of t-tests in Python, we’ll use the scipy.stats module, which provides functions for performing various statistical tests. Below are examples for both Independent Samples T-Test and Paired Samples T-Test, along with sample data.\n\nIndependent Samples T-Test\n\nThis test compares the means of two independent groups to determine if they are significantly different. For instance, comparing the average test scores of two different classes.\nExample:\nSuppose we have test scores from two classes, and we want to determine if there’s a significant difference between their average scores.\n\n\n\nDescription of the image\n\n\n\n\nA t-test is a statistical method used to determine whether there is a significant difference between the means of two groups. It’s commonly applied in various fields to compare group averages and assess the impact of interventions or treatments.\n\nimport numpy as np\nfrom scipy import stats\n\n# Sample data: test scores of two classes\nclass_A_scores = [85, 88, 90, 92, 86, 87, 91, 89, 84, 90]\nclass_B_scores = [78, 82, 80, 79, 81, 77, 83, 80, 78, 82]\n\n# Perform Independent Samples T-Test\nt_stat, p_value = stats.ttest_ind(class_A_scores, class_B_scores)\n\nprint(f\"T-Statistic: {t_stat:.2f}\")\nprint(f\"P-Value: {p_value:.4f}\")\n\n# Interpret the result\nalpha = 0.05\nif p_value &lt; alpha:\n    print(\"Reject the null hypothesis: There is a significant difference between the two classes.\")\nelse:\n    print(\"Fail to reject the null hypothesis: No significant difference between the two classes.\")\n\nT-Statistic: 7.79\nP-Value: 0.0000\nReject the null hypothesis: There is a significant difference between the two classes.\n\n\n\nIn this example, the p-value is less than the significance level (alpha = 0.05), indicating a significant difference between the average scores of the two classes.\nPaired Samples T-Test This test compares the means from the same group at different times or under different conditions. For example, measuring the weights of individuals before and after a diet program.\nExample:\nAssume we have the weights of individuals before and after a diet program, and we want to determine if the program had a significant effect\n\nPaired Samples T-Test\n\nThis test compares the means from the same group at different times or under different conditions. For example, measuring the weights of individuals before and after a diet program.\nExample:\nAssume we have the weights of individuals before and after a diet program, and we want to determine if the program had a significant effect.\n\n\nimport numpy as np\nfrom scipy import stats\n\n# Sample data: weights before and after a diet program\nweights_before = [200, 195, 180, 210, 190, 205, 185, 200, 195, 210]\nweights_after = [190, 188, 175, 200, 185, 198, 180, 195, 190, 205]\n\n# Perform Paired Samples T-Test\nt_stat, p_value = stats.ttest_rel(weights_before, weights_after)\n\nprint(f\"T-Statistic: {t_stat:.2f}\")\nprint(f\"P-Value: {p_value:.4f}\")\n\n# Interpret the result\nalpha = 0.05\nif p_value &lt; alpha:\n    print(\"Reject the null hypothesis: The diet program had a significant effect.\")\nelse:\n    print(\"Fail to reject the null hypothesis: No significant effect of the diet program.\")\n\nT-Statistic: 9.80\nP-Value: 0.0000\nReject the null hypothesis: The diet program had a significant effect.\n\n\n\nHere, the p-value is less than 0.05, suggesting that the diet program led to a significant reduction in weight.\nNote: Before performing t-tests, it’s essential to check the assumptions of normality and equal variances. If these assumptions are violated, consider using non-parametric tests or applying data transformations.\nThese examples illustrate how to perform t-tests in Python using sample data, helping to determine whether observed differences between groups are statistically significant."
  },
  {
    "objectID": "Top10_bioinfo_stats.html#probability-theory-and-bayes-theorem",
    "href": "Top10_bioinfo_stats.html#probability-theory-and-bayes-theorem",
    "title": "Frequently used statistical concepts in Bioinformatics",
    "section": "1. Probability Theory and Bayes’ Theorem",
    "text": "1. Probability Theory and Bayes’ Theorem\nDefinition: Probability theory quantifies the likelihood of events occurring. Bayes’ Theorem provides a way to update the probability of a hypothesis based on new evidence.\nMathematical Formulation: 𝑃 ( 𝐴 ∣ 𝐵 ) = 𝑃 ( 𝐵 ∣ 𝐴 ) × 𝑃 ( 𝐴 ) 𝑃 ( 𝐵 ) P(A∣B)= P(B) P(B∣A)×P(A)​Where:\n𝑃 ( 𝐴 ∣ 𝐵 ) P(A∣B) is the posterior probability of event A given B. 𝑃 ( 𝐵 ∣ 𝐴 ) P(B∣A) is the likelihood of event B given A. 𝑃 ( 𝐴 ) P(A) and 𝑃 ( 𝐵 ) P(B) are the prior probabilities of events A and B, respectively.\nExample: In genomics, determining the probability of a disease given a genetic marker involves updating prior knowledge with new genetic data\n\n\n# Calculating posterior probability using Bayes' Theorem\ndef bayes_theorem(prior_A, likelihood_B_given_A, prior_B):\n    return (likelihood_B_given_A * prior_A) / prior_B\n\n# Example values\nprior_disease = 0.01  # Prior probability of disease\nsensitivity = 0.9     # P(Test positive | Disease)\nspecificity = 0.95    # P(Test negative | No Disease)\nprior_no_disease = 1 - prior_disease\nfalse_positive_rate = 1 - specificity\n\n# P(Test positive)\nprior_test_positive = (sensitivity * prior_disease) + (false_positive_rate * prior_no_disease)\n\n# P(Disease | Test positive)\nposterior_disease_given_positive = bayes_theorem(prior_disease, sensitivity, prior_test_positive)\nprint(f\"Posterior probability of disease given a positive test: {posterior_disease_given_positive:.4f}\")\n\nPosterior probability of disease given a positive test: 0.1538"
  },
  {
    "objectID": "Top10_bioinfo_stats.html#hypothesis-testing",
    "href": "Top10_bioinfo_stats.html#hypothesis-testing",
    "title": "Frequently used statistical concepts in Bioinformatics",
    "section": "2. Hypothesis Testing",
    "text": "2. Hypothesis Testing\nDefinition: A statistical method to determine if there is enough evidence to reject a null hypothesis in favor of an alternative hypothesis.\nMathematical Formulation:\nNull Hypothesis (𝐻0): Assumes no effect or difference.\nAlternative Hypothesis (𝐻1): Assumes an effect or difference exists.\nTest Statistic: A value calculated from sample data used to decide whether to reject 𝐻0.\np-value: The probability of obtaining a test statistic at least as extreme as the one observed, assuming 𝐻0 is true.\nExample: Testing whether a new drug affects gene expression levels compared to a control.\n\n\nimport numpy as np\nfrom scipy import stats\n\n# Sample data: gene expression levels\ncontrol = np.array([5.1, 5.3, 5.5, 5.7, 5.9])\ntreatment = np.array([5.8, 6.0, 6.2, 6.4, 6.6])\n\n# Perform two-sample t-test\nt_stat, p_value = stats.ttest_ind(treatment, control)\nprint(f\"T-statistic: {t_stat:.4f}, p-value: {p_value:.4f}\")\n\n# Interpret the result\nalpha = 0.05\nif p_value &lt; alpha:\n    print(\"Reject the null hypothesis: Significant difference between groups.\")\nelse:\n    print(\"Fail to reject the null hypothesis: No significant difference between groups.\")\n\nT-statistic: 3.5000, p-value: 0.0081\nReject the null hypothesis: Significant difference between groups."
  },
  {
    "objectID": "Top10_bioinfo_stats.html#regression-analysis",
    "href": "Top10_bioinfo_stats.html#regression-analysis",
    "title": "Frequently used statistical concepts in Bioinformatics",
    "section": "3. Regression Analysis",
    "text": "3. Regression Analysis\nDefinition: A set of statistical processes for estimating relationships among variables.\nMathematical Formulation:\nLinear Regression Model:\n𝑌 = 𝛽 0 + 𝛽 1 𝑋 + 𝜖 Y=β 0 ​ +β 1 ​ X+ϵ 𝑌\nY: Dependent variable\nX: Independent variable\nβ 0 ​ : Intercept\nβ 1 ​ : Slope\nϵ: Error term\nExample: Predicting protein concentration based on gene expression levels.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Sample data: gene expression (X) and protein concentration (Y)\nX = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\nY = np.array([1.2, 1.9, 3.1, 3.9, 5.1])\n\n# Create and fit the model\nmodel = LinearRegression()\nmodel.fit(X, Y)\n\n# Coefficients\nintercept = model.intercept_\nslope = model.coef_[0]\nprint(f\"Intercept: {intercept:.2f}, Slope: {slope:.2f}\")\n\n# Predict and plot\nY_pred = model.predict(X)\nplt.scatter(X, Y, color='blue', label='Actual data')\nplt.plot(X, Y_pred, color='red', label='Fitted line')\nplt.xlabel('Gene Expression')\nplt.ylabel('Protein Concentration')\nplt.legend()\nplt.show()\n\nIntercept: 0.10, Slope: 0.98\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\n# Sample data: gene expression levels in different tissues\ndata = {\n    'Tissue': ['Liver', 'Liver', 'Liver', 'Heart', 'Heart', 'Heart', 'Brain', 'Brain', 'Brain'],\n    'Expression': [5.1, 5.3, 5.5, 6.1, 6.3, 6.5, 7.1, 7.3, 7.5]\n}\ndf = pd.DataFrame(data)\n\n# Perform one-way ANOVA\nliver = df[df['Tissue'] == 'Liver']['Expression']\nheart = df[df['Tissue'] == 'Heart']['Expression']\nbrain = df[df['Tissue'] == 'Brain']['Expression']\n\nf_stat, p_val = stats.f_oneway(liver, heart, brain)\nprint(f\"F-statistic: {f_stat:.4f}, p-value: {p_val:.4f}\")\n\n# Interpret the result\nalpha = 0.05\nif p_val &lt; alpha:\n    print(\"Reject the null hypothesis: Significant differences exist between tissue groups.\")\nelse:\n    print(\"Fail to reject the null hypothesis: No significant differences between tissue groups.\")\n\nF-statistic: 75.0000, p-value: 0.0001\nReject the null hypothesis: Significant differences exist between tissue groups.\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# Sample data: gene expression levels (rows: genes, columns: samples)\ndata = np.array([\n    [2.5, 2.4],\n    [0.5, 0.7],\n    [2.2, 2.9],\n    [1.9, 2.2],\n    [3.1, 3.0],\n    [2.3, 2.7],\n    [2.0, 1.6],\n    [1.0, 1.1],\n    [1.5, 1.6],\n    [1.1, 0.9]\n])\n\n# Standardize the data\nscaler = StandardScaler()\ndata_std = scaler.fit_transform(data)\n\n# Perform PCA\npca = PCA(n_components=2)\nprincipal_components = pca.fit_transform(data_std)\n\n# Plot the results\nplt.scatter(principal_components[:, 0], principal_components[:, 1], c='blue')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('PCA of Gene Expression Data')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\n# Sample data: gene expression levels\ndata = np.array([\n    [1.0, 2.0],\n    [1.5, 1.8],\n    [5.0, 8.0],\n    [8.0, 8.0],\n    [1.0, 0.6],\n    [9.0, 11.0],\n    [8.0, 2.0],\n    [10.0, 2.0],\n    [9.0, 3.0]\n])\n\n# Perform K-means clustering\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(data)\nlabels = kmeans.labels_\ncentroids = kmeans.cluster_centers_\n\n# Plot the results\ncolors = ['r', 'g', 'b']\nfor i in range(len(data)):\n    plt.scatter(data[i][0], data[i][1], c=colors[labels[i]], s=30)\nplt.scatter(centroids[:, 0], centroids[:, 1], marker='x', s=100, c='black')\nplt.xlabel('Gene Expression Feature 1')\nplt.ylabel('Gene Expression Feature 2')\nplt.title('K-means Clustering of Gene Expression Data')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Define states and transition matrix\nstates = ['A', 'C', 'G', 'T']\ntransition_matrix = {\n    'A': {'A': 0.3, 'C': 0.2, 'G': 0.2, 'T': 0.3},\n    'C': {'A': 0.1, 'C': 0.4, 'G': 0.4, 'T': 0.1},\n    'G': {'A': 0.2, 'C': 0.3, 'G': 0.3, 'T': 0.2},\n    'T': {'A': 0.25, 'C': 0.25, 'G': 0.25, 'T': 0.25},\n}\n\n# Generate a Markov sequence\ndef generate_markov_sequence(length, start_state='A'):\n    sequence = [start_state]\n    current_state = start_state\n    for _ in range(length - 1):\n        next_state = random.choices(\n            population=states,\n            weights=[transition_matrix[current_state][s] for s in states]\n        )[0]\n        sequence.append(next_state)\n        current_state = next_state\n    return ''.join(sequence)\n\n# Generate a sequence of length 50 starting with 'A'\nmarkov_sequence = generate_markov_sequence(50, start_state='A')\nprint(\"Generated Markov Sequence:\", markov_sequence)\n\nGenerated Markov Sequence: AAATCGTGTTTAATCGGCGACCGCCGTATCCCCGCCTGACGTTGGGAATG\n\n\n\nimport logging\n\n# Set logging level to suppress informational messages\nlogging.getLogger().setLevel(logging.ERROR)\n\n# Your code here\n\nfrom hmmlearn import hmm\n\n# Define the HMM\nmodel = hmm.MultinomialHMM(n_components=2, n_iter=100, tol=0.01)\n\n# Encoding Exon and Intron states as 0 and 1\n# Assume 'A', 'C', 'G', 'T' as observations (encoded as 0, 1, 2, 3)\nstates = ['Exon', 'Intron']\nobservations = ['A', 'C', 'G', 'T']\n\n# Transition probability matrix for Exon and Intron\n# High self-transition probabilities to simulate longer sequences\nmodel.startprob_ = np.array([0.5, 0.5])  # Start with equal probability\nmodel.transmat_ = np.array([\n    [0.8, 0.2],  # Exon to Exon, Exon to Intron\n    [0.2, 0.8]   # Intron to Exon, Intron to Intron\n])\n\n# Emission probability matrix for Exon and Intron\n# Exons may have slightly different nucleotide distribution\nmodel.emissionprob_ = np.array([\n    [0.25, 0.25, 0.25, 0.25],  # Equal for simplicity in exons\n    [0.1, 0.4, 0.4, 0.1]       # Higher C/G content in introns\n])\n\n# Generate a sample sequence (observations encoded as integers)\nsequence = np.array([[0, 1, 2, 3, 2, 1, 0, 2, 3, 0]]).T  # Sample DNA sequence as 'A', 'C', 'G', 'T'\n\n# Fit the model to the sequence and predict the hidden states\nmodel = model.fit(sequence)\nhidden_states = model.predict(sequence)\n\n# Decode and print the hidden states\ndecoded_states = [states[state] for state in hidden_states]\nprint(\"Observed Sequence: \", ''.join([observations[i[0]] for i in sequence]))\nprint(\"Predicted Hidden States:\", decoded_states)\n\nObserved Sequence:  ACGTGCAGTA\nPredicted Hidden States: ['Intron', 'Exon', 'Intron', 'Exon', 'Intron', 'Exon', 'Intron', 'Exon', 'Intron', 'Exon']\n\n\n\nimport numpy as np\nfrom statsmodels.stats.multitest import multipletests\n\n# Sample p-values from multiple tests\np_values = np.array([0.01, 0.04, 0.03, 0.05, 0.20, 0.001, 0.15, 0.005])\n\n# Apply Benjamini-Hochberg correction for False Discovery Rate (FDR)\nreject, pvals_corrected, _, _ = multipletests(p_values, alpha=0.05, method='fdr_bh')\n\n# Results\nfor i, (p_val, p_corr, rej) in enumerate(zip(p_values, pvals_corrected, reject)):\n    print(f\"Test {i+1}: Original p-value = {p_val:.3f}, Corrected p-value = {p_corr:.3f}, Reject null hypothesis: {rej}\")\n\nTest 1: Original p-value = 0.010, Corrected p-value = 0.027, Reject null hypothesis: True\nTest 2: Original p-value = 0.040, Corrected p-value = 0.064, Reject null hypothesis: False\nTest 3: Original p-value = 0.030, Corrected p-value = 0.060, Reject null hypothesis: False\nTest 4: Original p-value = 0.050, Corrected p-value = 0.067, Reject null hypothesis: False\nTest 5: Original p-value = 0.200, Corrected p-value = 0.200, Reject null hypothesis: False\nTest 6: Original p-value = 0.001, Corrected p-value = 0.008, Reject null hypothesis: True\nTest 7: Original p-value = 0.150, Corrected p-value = 0.171, Reject null hypothesis: False\nTest 8: Original p-value = 0.005, Corrected p-value = 0.020, Reject null hypothesis: True\n\n\n\nimport pandas as pd\nfrom lifelines import KaplanMeierFitter\nimport matplotlib.pyplot as plt\n\n# Sample data: survival times and event occurrences\ndata = {\n    'Time': [5, 6, 6, 2, 4, 4, 3, 5, 8, 6],\n    'Event': [1, 0, 1, 1, 0, 1, 0, 1, 1, 0]\n}\ndf = pd.DataFrame(data)\n\n# Initialize the Kaplan-Meier fitter\nkmf = KaplanMeierFitter()\n\n# Fit the data\nkmf.fit(durations=df['Time'], event_observed=df['Event'])\n\n# Plot the survival function\nkmf.plot_survival_function()\nplt.title('Survival Function')\nplt.xlabel('Time')\nplt.ylabel('Survival Probability')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create a sample directed graph representing a biological pathway\nG = nx.DiGraph()\n\n# Add nodes (genes/proteins)\nnodes = ['GeneA', 'GeneB', 'GeneC', 'GeneD', 'GeneE']\nG.add_nodes_from(nodes)\n\n# Add edges (interactions)\nedges = [('GeneA', 'GeneB'), ('GeneB', 'GeneC'), ('GeneC', 'GeneD'), ('GeneD', 'GeneE'), ('GeneA', 'GeneC')]\nG.add_edges_from(edges)\n\n# Draw the network\npos = nx.spring_layout(G)\nnx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=2000, font_size=10, font_weight='bold')\nplt.title('Biological Pathway Network')\nplt.show()"
  },
  {
    "objectID": "genomics-blog.html",
    "href": "genomics-blog.html",
    "title": "AI for Genomics",
    "section": "",
    "text": "Welcome to my blog. Here, I share my thoughts on various topics.\n\nT-test Applications\nTop 10 statistical concepts used in Bioiformatics"
  },
  {
    "objectID": "ml-blog.html",
    "href": "ml-blog.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Welcome to my blog. Here, I share my thoughts on various topics.\n\nT-test Applications\nTop 10 statistical concepts used in Bioiformatics"
  }
]